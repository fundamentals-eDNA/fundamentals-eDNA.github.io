---
title: Reflection on standardization
author: Alex Ascher
description: Week 5 blog post
---
Standardization of eDNA techniques is a lofty goal. There is clear sense in setting out standardized protocols for collection, storage, extraction, and amplification. Studies have shown that altering pieces of these steps can have affects on your results, such as Deiner et al where taxa richness and DNA concentration differed between different capture and extraction protocols. We are still in the early days of eDNA science, and reproducibility goes a long way towards increasing trust in this technology. The major issue is that DNA can be very sensitive to a slew of external factors, and ultimately the protocols we use will be the ones that work the best for our particular study. Additionally, funding can be an issue when it comes to expensive kits and reactants such as primer sets, polymerases, or blockers. Common sense dictates that simple steps such as collection and storage can and should be standardized, certainly across the same study if not more widely (e.g. across institutions). As this type of research gains in popularity we may find that certain techniques in general become refined down to the point that across the entire field best practices can be established. However, as I gain more hands on experience with eDNA it seems clear that the nitty-gritty of extraction, primer, or blocker use is difficult to make broadly applicable, and may frequently need to be tailored to individual studies. I think that is part of the reason we see so many eDNA methodology papers as scientists experiment with different techniques to get desired results. With any luck Steven is right and machine learning will make all of this relatively moot.